#CNN

#特征图与感受野
"""
特征图：二维卷积层输出的二维数组可以看做是输入在空间维度(宽和高)上，某一级的表征，也叫做特征图
感受野：影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做x的感受野
(可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征)
"""

#填充和步长
"""
卷积层的两个超参数：填充和步长
1.不填充：
n-k+1
2.有填充：
n-k+p+1 这里的p是两侧一共填充p
在很多情况下，会设置p=k-1来使得输入和输出具有相同的高和宽，这样会方便在构造网络时推测每个层的输出形状。如果k是奇数，则在两侧分别填充p/2,如果k是偶数，则在两侧其中的一侧填充向上取整(p/2),在另一侧填充向下取整(p/2)
一般来说CNN会选择具有奇数尺寸的卷积核
3.步长不唯一
(n-k+p+s)/s
"""
#nn.Conv2d的设置
"""
可以设置有输入通道，输出通道、卷积核尺寸和填充大小
nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,padding=1,stride=1)
nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(3,5),padding=(1,2),stride=(1,2))
值得注意的是，一般说的都是H*W
默认情况下，填充为0，步长为1
"""

#多输入通道和多输出通道
"""
对于多通道数据，进行卷积时，会为每个输入通道各分分配一个形状为该卷积核大小的二维卷积核，实际上就是得到一个c*H*W的卷积核，输入与卷积核按通道分别做卷积运算，然后按通道进行相加后输出，得到一个二维数组，即为卷积输出
这里值得注意的是，一个多通道卷积核，每个通道的元素数值可以是不一样的(并不一定是二维数据重复(通道数)次)
上述：当输入通道有多个时，因为对各个通道的结果做了累加，所以不论输入通道数是多少，输出通道数总是1,
"""

#池化
"""
池化层的目的：为了缓解卷积层对位置的过度敏感性
nn.MaxPool2d(size,padding,stride)
对于多通道池化，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加，即池化层的输出通道上述与输入通道数相等
"""


import torch
import torch.nn as nn
import numpy as np
def comp_conv2d(conv2d,X): #函数作为参数传递，传函数名就可以了
    X=X.view((1,1)+X.shape) #值得注意的是，这里的(1,1)+X.shape，相当于增加了两个维度，增加了批量的大小和通道数,因为nn.Conv2d接收的是4Dtensor
    Y=conv2d(X)
    return Y.view(Y.shape[2:]) #排除不关心的前两维，批量和通道

#填充
conv2d=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=3,padding=1)
X=torch.rand(8,8)
print(comp_conv2d(conv2d,X))


